{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6020138a-0dc8-4b7e-bad8-9cf2ef7133aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark import StorageLevel \n",
    "from pyspark.sql.functions import rand, current_date, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b24dc6e-9c01-4a8b-9b6e-0d71b953bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 13:44:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"cache-and-persist\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512m\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6169b6-ba60-4544-bf4f-b58cbb2e3d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure the execution time of a query\n",
    "import time\n",
    "\n",
    "def measure_time(query):\n",
    "    start = time.time()\n",
    "    query.collect() # Force the query execution by calling an action\n",
    "    end = time.time()\n",
    "    print(f\"Execution time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d243df0-aa88-4ea7-a363-76e00eafc3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "| id|      date|ProductId|\n",
      "+---+----------+---------+\n",
      "|  0|2024-02-10|       67|\n",
      "|  1|2023-07-12|       39|\n",
      "|  2|2023-08-10|        8|\n",
      "|  3|2023-05-22|       29|\n",
      "|  4|2023-06-22|       63|\n",
      "+---+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create some sample data frames\n",
    "# A large data frame with 10 million rows and two columns: id and value\n",
    "large_df = (spark.range(0, 10000000)\n",
    "            .withColumn(\"date\", date_sub(current_date(), (rand() * 365).cast(\"int\")))\n",
    "            .withColumn(\"ProductId\", (rand() * 100).cast(\"int\")))\n",
    "large_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb8e6b1-94fa-4d45-9b95-ef6887b8e4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame using cache() method\n",
    "large_df.cache()\n",
    "# Check the storage level of the cached DataFrame\n",
    "print(large_df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d80802-2095-47fa-83be-672a56f8c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# Persist the DataFrame using persist() method with a different storage level\n",
    "large_df.persist(StorageLevel.MEMORY_AND_DISK_DESER)\n",
    "# Check the storage level of the persisted DataFrame\n",
    "print(large_df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760553dd-2781-4b03-9e68-7c7442e6dc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8.600075006484985 seconds\n",
      "+---------+---------+\n",
      "|ProductId|count(Id)|\n",
      "+---------+---------+\n",
      "|       31|    99961|\n",
      "|       85|    99746|\n",
      "|       65|   100023|\n",
      "|       53|   100615|\n",
      "|       78|    99985|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = large_df.groupBy(\"ProductId\").agg({\"Id\": \"count\"}) \n",
    "measure_time(results_df)\n",
    "# Show the result\n",
    "results_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58fe1655-0f67-481c-8fc0-061ea5360e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.984121561050415 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|ProductId|count(Id)|\n",
      "+---------+---------+\n",
      "|       31|    99961|\n",
      "|       85|    99746|\n",
      "|       65|   100023|\n",
      "|       53|   100615|\n",
      "|       78|    99985|\n",
      "+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_df = large_df.groupBy(\"ProductId\").agg({\"Id\": \"count\"}) \n",
    "measure_time(results_df)\n",
    "# Show the result\n",
    "results_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b5a908-2260-467c-96a3-6245ebba3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "# Unpersist the DataFrame using unpersist() method\n",
    "large_df.unpersist()\n",
    "# Check the storage level of the unpersisted DataFrame\n",
    "print(large_df.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e360de96-05cf-4575-92a4-567200d91f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55fe9f-b1e7-4885-92f2-d48e758928c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
