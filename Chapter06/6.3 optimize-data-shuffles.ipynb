{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d05d7a1-fe97-491a-a177-c1886a5f8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import col, avg, date_sub, current_date, rand, when, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bfbe105-2fd1-43f1-95e9-525d85226a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"optimize-data-shuffles\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512m\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Set log level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae2e1f7-45c3-486a-970e-0727eb303197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+------+------+-----+\n",
      "| id|      date|age|salary|gender|grade|\n",
      "+---+----------+---+------+------+-----+\n",
      "|  0|2023-06-22| 50|  3600|     F| IC-3|\n",
      "|  1|2023-03-19| 35|  2500|     F|   IC|\n",
      "|  2|2023-06-17| 77|  3400|     F|   M1|\n",
      "|  3|2023-03-16| 97|  2600|     M|   IC|\n",
      "|  4|2024-01-03| 87|  5400|     F|   M1|\n",
      "+---+----------+---+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create some sample data frames\n",
    "# A large data frame with 1 million rows and two columns: id and value\n",
    "large_df = (spark.range(0, 1000000)\n",
    "            .withColumn(\"date\", date_sub(current_date(), (rand() * 365).cast(\"int\")))\n",
    "            .withColumn(\"age\", (rand() * 100).cast(\"int\"))\n",
    "            .withColumn(\"salary\", 100*(rand() * 100).cast(\"int\"))\n",
    "            .withColumn(\"gender\", when((rand() * 2).cast(\"int\") == 0, \"M\").otherwise(\"F\"))\n",
    "            .withColumn(\"grade\", \n",
    "                        when((rand() * 5).cast(\"int\") == 0, \"IC\")\n",
    "                        .when((rand() * 5).cast(\"int\") == 1, \"IC-2\")\n",
    "                        .when((rand() * 5).cast(\"int\") == 2, \"M1\")\n",
    "                        .when((rand() * 5).cast(\"int\") == 3, \"M2\")\n",
    "                        .when((rand() * 5).cast(\"int\") == 4, \"IC-3\")\n",
    "                        .otherwise(\"M3\")))\n",
    "large_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e1b2f38-375a-4c39-94ab-45597162caf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|age|       avg(bonus)|\n",
      "+---+-----------------+\n",
      "| 85|5462.653508771929|\n",
      "| 65|5478.981668009669|\n",
      "| 78|5457.682827459767|\n",
      "| 81| 5482.96132596685|\n",
      "| 76| 5465.81568744408|\n",
      "+---+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame by age\n",
    "df_filtered = large_df.filter(col(\"age\") >= 55)\n",
    "\n",
    "# Map the DataFrame by adding 10% bonus to salary\n",
    "df_mapped = df_filtered.withColumn(\"bonus\", col(\"salary\") * 1.1)\n",
    "\n",
    "# Locally aggregate the DataFrame by computing the average bonus by age\n",
    "df_aggregated = df_mapped.groupBy(\"age\").agg(avg(\"bonus\"))\n",
    "\n",
    "# Print the result\n",
    "df_aggregated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2492f49-744d-44ea-a253-1b73c1d04710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[age#9], functions=[avg(bonus#56)])\n",
      "   +- Exchange hashpartitioning(age#9, 200), ENSURE_REQUIREMENTS, [plan_id=123]\n",
      "      +- HashAggregate(keys=[age#9], functions=[partial_avg(bonus#56)])\n",
      "         +- Project [age#9, (cast(salary#13 as double) * 1.1) AS bonus#56]\n",
      "            +- Filter (isnotnull(age#9) AND (age#9 >= 55))\n",
      "               +- Project [age#9, (cast((rand(900612033348343497) * 100.0) as int) * 100) AS salary#13]\n",
      "                  +- Project [cast((rand(3280734957678084291) * 100.0) as int) AS age#9]\n",
      "                     +- Range (0, 1000000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aggregated.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480aba43-0e1d-4f84-82e0-c7cdaa9debb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|level|       avg(salary)|\n",
      "+-----+------------------+\n",
      "|    F| 5002.275473217882|\n",
      "|    E|  4973.83709120455|\n",
      "|    B| 4939.277204130262|\n",
      "|    D| 4983.376623376624|\n",
      "|    C| 4948.175987171778|\n",
      "|    J|            4956.3|\n",
      "|    A| 4930.878955298845|\n",
      "|    G| 4923.778271405493|\n",
      "|    I| 4980.892425463336|\n",
      "|    H|4999.8611662038875|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create another DataFrame with some dummy data\n",
    "df2 = spark.createDataFrame([(25, \"A\"), (30, \"B\"), (35, \"C\"), (40, \"D\"), (45, \"E\"), (50, \"F\"), (55, \"G\"), (60, \"H\"), (65, \"I\"), (70, \"J\")], [\"age\", \"level\"])\n",
    "\n",
    "# Join the two DataFrames by age using broadcast join\n",
    "df_joined = large_df.join(broadcast(df2), \"age\")\n",
    "\n",
    "# Globally aggregate the joined DataFrame by computing the sum of salary by level using partial aggregation\n",
    "df_aggregated = df_joined.groupBy(\"level\").avg(\"salary\")\n",
    "\n",
    "# Print the result\n",
    "df_aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc13c8b-a1fc-41b1-b0b2-06f00ff5e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[level#91], functions=[avg(salary#13)])\n",
      "   +- Exchange hashpartitioning(level#91, 200), ENSURE_REQUIREMENTS, [plan_id=311]\n",
      "      +- HashAggregate(keys=[level#91], functions=[partial_avg(salary#13)])\n",
      "         +- Project [salary#13, level#91]\n",
      "            +- BroadcastHashJoin [cast(age#9 as bigint)], [age#90L], Inner, BuildRight, false\n",
      "               :- Filter isnotnull(age#9)\n",
      "               :  +- Project [age#9, (cast((rand(900612033348343497) * 100.0) as int) * 100) AS salary#13]\n",
      "               :     +- Project [cast((rand(3280734957678084291) * 100.0) as int) AS age#9]\n",
      "               :        +- Range (0, 1000000, step=1, splits=2)\n",
      "               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=306]\n",
      "                  +- Filter isnotnull(age#90L)\n",
      "                     +- Scan ExistingRDD[age#90L,level#91]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aggregated.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbf4ad27-0f23-4304-8b99-cf34408e3b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition the DataFrame by gender with 2 partitions\n",
    "df_repartitioned = large_df.repartition(col(\"gender\"))\n",
    "\n",
    "# Repartition the DataFrame by age range with 5 partitions\n",
    "df_repartitioned_by_range = large_df.repartitionByRange(5, col(\"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf5ea6f-0ef3-4938-ac60-140b857e051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#4L, date#6, age#9, salary#13, gender#18, CASE WHEN (cast((rand(-5258595656362598529) * 5.0) as int) = 0) THEN IC WHEN (cast((rand(6677297146942895454) * 5.0) as int) = 1) THEN IC-2 WHEN (cast((rand(6084707916817199194) * 5.0) as int) = 2) THEN M1 WHEN (cast((rand(7628756694394173931) * 5.0) as int) = 3) THEN M2 WHEN (cast((rand(-1822131519618029291) * 5.0) as int) = 4) THEN IC-3 ELSE M3 END AS grade#24]\n",
      "+- *(1) Project [id#4L, date#6, age#9, salary#13, CASE WHEN (cast((rand(-1512255023260467776) * 2.0) as int) = 0) THEN M ELSE F END AS gender#18]\n",
      "   +- *(1) Project [id#4L, date#6, age#9, (cast((rand(900612033348343497) * 100.0) as int) * 100) AS salary#13]\n",
      "      +- *(1) Project [id#4L, date#6, cast((rand(3280734957678084291) * 100.0) as int) AS age#9]\n",
      "         +- *(1) Project [id#4L, date_sub(2024-02-21, cast((rand(-5065184338059177050) * 365.0) as int)) AS date#6]\n",
      "            +- *(1) Range (0, 1000000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "large_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4327c59e-007c-4b64-b9a3-d6f800d0ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange hashpartitioning(gender#18, 200), REPARTITION_BY_COL, [plan_id=363]\n",
      "   +- Project [id#4L, date#6, age#9, salary#13, gender#18, CASE WHEN (cast((rand(-5258595656362598529) * 5.0) as int) = 0) THEN IC WHEN (cast((rand(6677297146942895454) * 5.0) as int) = 1) THEN IC-2 WHEN (cast((rand(6084707916817199194) * 5.0) as int) = 2) THEN M1 WHEN (cast((rand(7628756694394173931) * 5.0) as int) = 3) THEN M2 WHEN (cast((rand(-1822131519618029291) * 5.0) as int) = 4) THEN IC-3 ELSE M3 END AS grade#24]\n",
      "      +- Project [id#4L, date#6, age#9, salary#13, CASE WHEN (cast((rand(-1512255023260467776) * 2.0) as int) = 0) THEN M ELSE F END AS gender#18]\n",
      "         +- Project [id#4L, date#6, age#9, (cast((rand(900612033348343497) * 100.0) as int) * 100) AS salary#13]\n",
      "            +- Project [id#4L, date#6, cast((rand(3280734957678084291) * 100.0) as int) AS age#9]\n",
      "               +- Project [id#4L, date_sub(2024-02-21, cast((rand(-5065184338059177050) * 365.0) as int)) AS date#6]\n",
      "                  +- Range (0, 1000000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_repartitioned.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc1d6cf-d898-414c-8e49-07f91160d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Exchange rangepartitioning(age#9 ASC NULLS FIRST, 5), REPARTITION_BY_NUM, [plan_id=391]\n",
      "   +- Project [id#4L, date#6, age#9, salary#13, gender#18, CASE WHEN (cast((rand(-5258595656362598529) * 5.0) as int) = 0) THEN IC WHEN (cast((rand(6677297146942895454) * 5.0) as int) = 1) THEN IC-2 WHEN (cast((rand(6084707916817199194) * 5.0) as int) = 2) THEN M1 WHEN (cast((rand(7628756694394173931) * 5.0) as int) = 3) THEN M2 WHEN (cast((rand(-1822131519618029291) * 5.0) as int) = 4) THEN IC-3 ELSE M3 END AS grade#24]\n",
      "      +- Project [id#4L, date#6, age#9, salary#13, CASE WHEN (cast((rand(-1512255023260467776) * 2.0) as int) = 0) THEN M ELSE F END AS gender#18]\n",
      "         +- Project [id#4L, date#6, age#9, (cast((rand(900612033348343497) * 100.0) as int) * 100) AS salary#13]\n",
      "            +- Project [id#4L, date#6, cast((rand(3280734957678084291) * 100.0) as int) AS age#9]\n",
      "               +- Project [id#4L, date_sub(2024-02-21, cast((rand(-5065184338059177050) * 365.0) as int)) AS date#6]\n",
      "                  +- Range (0, 1000000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_repartitioned_by_range.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "526788c3-4a1d-4314-b2cc-b8f3c13683c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c55186-dfad-4e3d-ba05-7b771dd33bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
